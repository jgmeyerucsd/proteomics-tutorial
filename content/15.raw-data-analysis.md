## 13. Raw Data Analysis {.page_break_before}

The goal of raw data analysis is to convert raw spectral data into lists of altered protein groups, which requires many steps, including checking data quality, peptide spectra matching, protein inference [@PMID:16009968; @DOI:10.1016/j.jprot.2016.08.002], quantification, and statistical hypothesis tests. 
Subsequently, many additional analyses can be performed to make biological inferences, which is covered in a subsequent section. 
An overview of the entire data analysis cycle is shown in (**Figure 16**). 

![**Proteomics Data Analysis and Biological Interpretation.**
The process begins with protein identification and quantification using tools such as Proteome Discoverer, Spectronaut, Spectromine, MS Fragger, MaxQuant, and Skyline. 
Quality control measures ensure data integrity, leading to a biological interpretation of the results. 
Differential expression analyses may include relative abundance charts, heat maps, and volcano plots. 
Functional analysis encompasses gene ontology, protein-protein interactions, and signaling pathways.
](images/Fig_12_Biological_interpretation.png){#fig:DIA-types tag="16" width="100%"}

Due to the inherent differences in the data structures of DDA and DIA measurements, there exist different types of software that can facilitate the steps mentioned above. 
The existing software for DDA and DIA analysis can be further divided into freeware and non-freeware.

### Analysis of DDA data
DDA data analysis either directly uses the vendor proprietary data format directly with a proprietary search engine like Mascot, SEQUEST (through Proteome Discoverer), Paragon (through Protein Pilot), or it can be processed through one of the many freely available search engines or pipelines, for example, Comet, MaxQuant, MSGF+, X!Tandem, Morpheus, MS-Fragger, and OMSSA. 
Table 7 gives weblinks and citations for these software tools.
For analysis with freeware, raw data is converted to standard open XML formats like mzML [@DOI:10.1074/mcp.R110.000133; @PMID:20013381; @DOI:10.1074/mcp.R112.019695]. 
The appropriate FASTA file containing proteins predicted from that organism's genome is chosen as a reference database to search the experimental spectra.
All search parameters like peptide and fragment mass errors (i.e. MS1 and MS2 tolerances), enzyme specificity, number of missed cleavages, chemical artifacts and potential biological modifications (variable/dynamic modifications) are specified before executing the search.
The search algorithm scores each query spectrum against its possible peptide matches [@DOI:10.1002/mas.21543]. 
A spectrum and its best scoring candidate peptide are called a peptide spectrum match (PSM).
The scores reflect a _goodness-of-fit_ between an experimental spectrum and a theoretical one and do not necessarily depict the correctness of the peptide assignment.

Table 7: DDA data analysis software examples.

|   Name   |         Publication           |             Website               |  Free/Paid   |
|:--------:|:-----------------------------:|:---------------------------------:|:------------:|
| MaxQuant |Cox and Mann, 2008[@DOI:10.1038/nbt.1511]|[MaxQuant](https://www.maxquant.org/)    |
| MSFragger|   Kong et al., 2017[@DOI:10.1038/nmeth.4256]    |    [MSFragger](https://msfragger.nesvilab.org/) | free |
| Mascot   |Perkins et al., 1999[@PMID:10612281]             |    [Mascot](https://www.matrixscience.com/)     |free |
| MS-GF+   |Kim et al., [@DOI:10.1038/ncomms6277]            | [MS-GF+](https://github.com/MSGFPlus/msgfplus)  |free |
| X!Tandem |Craig et al., [@DOI:10.1002/rcm.1198;@DOI:10.1093/bioinformatics/bth092]  | [GPMDB](https://www.thegpm.org/tandem/) |free |
| Comet    | Eng et al., 2012[@DOI:10.1002/pmic.201200439] | [Comet](https://uwpr.github.io/Comet/) |free |
| Skyline|MacLean et al., 2010[@DOI:10.1093/bioinformatics/btq054]|[Skyline](https://skyline.ms/project/home/software/Skyline/begin.view) |free |
| ProteomeDiscoverer ||[ProteomeDiscoverer](https://www.thermofisher.com/ch/en/home/industrial/mass-spectrometry/liquid-chromatography-mass-spectrometry-lc-ms/lc-ms-software/multi-omics-data-analysis/proteome-discoverer-software.html)| paid  |
| Mascot             |Perkins et al., 1999[@PMID:10612281]             |    [Mascot](https://www.matrixscience.com/)     |paid  |
| Spectromine   |           |    [Spectromine](ttps://biognosys.com/software/spectromine/?gclid=Cj0KCQiAoY-PBhCNARIsABcz770mjUz6iavBr9Ql7RPUdMvaHu9RYgPNrEfZco1wExEeoFwnQXuCHscaAlgBEALw_wcB)     |paid  |
| PEAKS   |Tran et al., 2018[@DOI:10.1038/s41592-018-0260-3] | [PEAKS](https://www.bioinfor.com/peaks-studio/)  | paid  |

Recall that we noted the stochasticity of DDA; every injection will select different peptide precursors for fragmentation leading to different identifications from each sample. 
To ameliorate this issue, often strategies are used to transfer identifications between multiple sample analyses. 
This transfer of IDs across runs is known as "match between runs", which was originally made famous by the processing software MaxQuant [@DOI:10.1074/mcp.M113.031591; @DOI:10.1038/nprot.2016.136]. 
There are several other similar tools and strategies, including the accurate mass and time approach [@DOI:10.1002/mas.20071], Q-MEND [@DOI:10.1021/pr0606880], IDEAL-Q [@DOI:10.1074/mcp.M900177-MCP200] and superHIRN [@DOI:10.1002/pmic.200700057].
More recent work has introducted statistical assessment of MBR methods using a two-proteome model [@DOI:10.1021/acs.jproteome.9b00492].
Statistically controlled MBR is currently available in the IonQuant tool [@DOI:10.1016/j.mcpro.2021.100077].


### Strategies for analysis of DIA data
DIA data analysis is fundamentally different from DDA data analysis because, instead of a single MS/MS spectrum for each peptide, we can observe the elution of peptide fragments for any peptide over chromatography time.
Even though DIA data analysis derives peptide matches differently, the same target-decoy analysis described above is often used. 
There are two general approaches for peptide identification from DIA data: peptide-centric and spectrum-centric.

Peptide-centric approaches looks for evidence of specific peptides that are in some assay library of MS/MS spectra. 
That library could be predicted spectra (e.g., using Prosit) [@DOI:10.1038/s41592-019-0426-7], or previously measured spectra (e.g., from a organism-wide knowledge base) [@PMID:30172843].
Examples of software that perform peptide-centric analysis include OpenSWATH [@DOI:10.1038/nbt.2841], Spectronaut [@DOI:10.1074/mcp.RA117.000314], csoDIAq [@DOI:10.1021/acs.analchem.1c02021], and DIA-NN [@DOI:10.1038/s41592-019-0638-x]. 

Spectrum-centric approaches instead ask if there is evidence for any peptide based on analysis of the observed spectra. 
Examples of spectrum-centric approaches include DIA-Umpire [@DOI:10.1038/nmeth.3255] and PECAN [@DOI:10.1038/nmeth.4390]. 
Spectrum-centric approaches may assemble pseudo-MS/MS spectra from co-elution of fragments that can then be used with any DDA database search [@DOI:10.1038/nmeth.3255].
Spectrum-centric approaches may be less sensitive at peptide identification than peptide-centric approaches. 

A non-comprehensive list of software for DIA data analysis is found in table 8.

Table 8: DIA data analysis software examples.

|   Name   |         Publication           |             Website               |  Free/Paid   |
|:--------:|:-----------------------------:|:---------------------------------:|:------------:|
| MaxDIA |Cox and Mann, 2008[@DOI:10.1038/nbt.1511]|        [MaxQuant](https://www.maxquant.org/)    | free |
| Skyline|MacLean et al., 2010[@DOI:10.1093/bioinformatics/btq054]|[Skyline](https://skyline.ms/project/home/software/Skyline/begin.view) |free |
| DIA-NN|Demichev et al., 2019[@DOI:10.1038/s41592-019-0638-x]  |    [DIA-NN](https://github.com/vdemichev/DiaNN)     |free |
| EncyclopeDIA | Searle et al., 2018[@DOI:10.1038/s41467-018-07454-w] | [EncyclopeDIA](http://www.searlelab.org/software/encyclopedia/index.html) |free |
| Spectronaut |Bruderer et al., 2015[@DOI:10.1074/mcp.M114.044305]|[Spectronaut](https://biognosys.com/software/spectronaut/)| paid |
| PEAKS   |Tran et al., 2018[@DOI:10.1038/s41592-018-0260-3] | [PEAKS](https://www.bioinfor.com/peaks-studio/)  | paid |
| Scaffold DIA   |     | [Proteome Software](https://www.proteomesoftware.com/products/scaffold-dia)  | paid |


### Deriving statistical significance of PSMs with the target-decoy approach
For evaluating the probability that a PSM is not random, matches to a decoy database of shuffled or reversed sequences are used as the null model for peptide matching.
A randomized or reversed version of target database is used as a nonparametric null model.
The decoy database can be searched separate from the target database (Kall's method)[@DOI:10.1021/pr700600n] or it can be combined with the target database before search (Elias and Gygi method)[@PMID:17327847].
Using either separate method or concatenated database search method, an estimate of false hits can be calculated which is used to estimate the false discovery rate (FDR) [@DOI:10.1007/978-1-4939-3106-4_7]. 
The FDR denotes the proportion of false hits in the population accepted as true.
For Kall's method: the false hits are estimated to be the number of decoys above a given threshold. 
It is assumed that the number of decoy hits that pass a threshold are the false hits. 
A similar number of target population may also be false. 
Therefore, the FDR is calculated as [@DOI:10.1021/acs.jproteome.6b00144]:

$$FDR = \frac{Decoy PSMs + 1}{Target PSMs}$$

For Elias and Gygi Method, the target population in which FDR is estimated changes.
The target and decoy hits coming from a joint database compete against each other.
For any spectrum, either a target or a decoy peptide can be the best hit.
It is argued that the joint target-decoy population has decoy hits as confirmed false hits.
However, due to the joint database search, the target database may also have equal number of false hits. 
Thus, the number of false hits is multiplied by two for FDR estimation.

$$FDR = \frac{2 * Decoy PSMs}{Target + Decoy PSMs}$$


### Integrated MS Data Analysis Platforms
Given the complexity of proteomic data analysis and the requirement for many steps to get from raw data to quantified proteins, there are some integrated software enviroments that easily allow users to complete everything in one place. 

#### Peptide Shaker 
Since each search engine may give slightly different results, and peptides identified by multiple search engines may be more confident hits, integration platforms such as PeptideShaker have been developed to combine search results [@DOI:10.1038/nbt.3109; @DOI:10.1021/acs.jproteome.1c00678]. 
PeptideShaker gives an interactive overview of all the protein, peptide, and PSMs in a dataset. 
It also has many other features, such as PTM summaries, 3D structure mapping of detected peptides, QC, validation, and GO enrichment (see biological interpretation section for more details). 

#### Trans-Proteomic Pipeline (TPP)
The Trans-Proteomic Pipeline (TPP) is a free and open-source mass spectrometry data analysis suite for end-to end analysis that remains in continual development to provide ever expansive data analysis capabilities since its inception over twenty years ago [@PMID:16729052; @PMID:20013374; @PMID:21082435; @PMID:21876204; @PMID:25418363; @PMID:25631240; @PMID:26419769; @PMID:29400476; @PMID:31290668; @PMID:36629399; @PMID:36648445]. 
The current release provides tools for mass spectrometry spectral processing, spectrum searching, search validation, abundance computation, protein inference, and statistical evaluation of the data to ensure controlled false-discovery rates. 
Many of the tools include machine-learning modeling to extract the most information from datasets and build robust statistical models to compute probabilities that derived information is correct.

One of the major advantages of TPP is its ability to be deployed in a wide variety of environments, from personal Windows laptops to extensive large Linux clusters for automated use within cloud computing environments. 
While the command-line interfaces are appreciated by many power users, others prefer a graphical user interface (GUI), which is provided by the TPP GUI called Petunia, allowing users to use the TPP from any web browser on any platform. 
Petunia has the advantages that the same exact GUI is available on a modest Windows laptop, a powerful expandable Linux server shared by a research group, or a remote cloud computing instance running on Amazon Web Services (AWS) [@PMID:25418363].

The TPP incudes many statistical validation tools such as PeptideProphet [@PMID:12403597], ProteinProphet [@PMID:14632076], iProphet [@PMID:21876204], and PTMProphet [@PMID:31290668], where Bayesian machine learning techniques are applied to the various search engine scores to model the correct and incorrect assignment distributions and then use these models to assign a probability of being correct based on these learned models. 
With these tools it is possible to validate search engine results on large-scale datasets and in short order, enabling users to select probability thresholds based on a selected tolerable false discovery rate (FDR). 
The TPP is made fully interoperable via the open XML-based formats pepXML and protXML for different aspects of processing data-dependent acquisition (DDA), and Data-independent acquisition (DIA) proteomics data, resulting in a complete suite of tools for processing the increasingly larger datasets from start to finish. 

DIA workflows are supported via the DISCO tool which reads mzML files containing the instrument-produced spectra and uses signal processing approaches to isolate the fragment ions in the multiplexed MS2 spectra that correlate with precursors in the MS1 and writes the results to new mzML files that may then be searched with standard DDA search engines and downstream tools, including target-decoy analysis. 
This provides a comprehensive analysis of DIA data without the need for building a spectral library first.

From its inception, the TPP has been and will always be free and open-source software, allowing anyone to use it without cost and to inspect its source code, alter the source code for their own needs, or even incorporate parts of it into their own products. 
Others have performed these tasks and include various analysis routines as addons such as TAILS N-terminomics analysis [@PMID:23667905], quantitation analysis with PyQuant [@PMID:27231314], SimPhospho [@PMID:29596608], WinProphet [@PMID:31305071], ProtyQuant [@PMID:32956841], and inclusion of R-tools for metaproteomic analysis [@PMID:36674563]. 
As a collection of individual tools, they are easily amenable to pipelining in a very flexible manner to support a huge variety of combinations and workflows, and a custom program may easily be inserted into the pipeline to support technology development.

##### Search engines supported by TPP
The heart of MS proteomics DDA data continues to be the “search engine” that interprets collections of mass spectra to determine the peptide or peptides that yielded them. 
Spectral library search engines and de novo search engines, which are less common, are also available and are included in software suites such as the Trans Proteomic Pipeline.
A sequence search engine most commonly used is the open-source version of SEQUEST called Comet, which is actively maintained and updated with new functionality as needs arise. 
For spectral library searching, SpectraST uses an approach where new spectra are matched against a library of previously identified spectra in the form of a spectral library [@PMID:18806791]. 
This approach is much faster, more sensitive, and more specific than sequence database searching, although is only as good as the reference spectral library provided. 
There is renewed interest in spectral libraries because of data-independent acquisition (DIA) approaches being increasingly deployed and therefore the quality and coverage of libraries is paramount and likely to improve in the coming years, aided by the standard spectral library format being developed by the PSI [@URL:https://psidev.info/mzSpecLib]. 
For de novo sequence analysis, Novor [@PMID:26122521] and Casanovo [@DOI:10.1101/2022.02.07.479481] are very fast and capable de novo sequence search engines that are available. 

For chemical crosslinking proteomics analysis, open-source programs such as Kojak [@PMID:25812159; @PMID:36629399] are available for standard or cleavable MS2-based crosslinking techniques. 
Crosslinking-based MS analyses are employed to elucidate protein-protein interactions and facilitate protein structure and topology predictions. 
Kojak is designed to identify two independent peptides covalently bonded with a crosslinker and fragmented in a single MS2 scan event using a database search approach. 
Kojak algorithm also includes support for cleavable cross-linkers, and identification of cross-links between 15N-labeled homomultimers and is integrated into the Trans-Proteomic Pipeline, enabling access to dozens of additional tools, in particular, the PeptideProphet and iProphet tools for validation of cross-links improve the sensitivity and accuracy of correct cross-link identifications at user-defined thresholds. 
Development of Kojak has continued over the last ten years culminating in many improvements and new features. 
These improvements include support for additional open formats and standards, further refinement to the search algorithm for efficiency, E-values to normalize the scores of the results, support for cleavable cross-linkers, and methods to identify cross-links between homomultimer subunits. 

For open modification database searching, programs such as Magnum (@PMID:35184559) are also now available which is specialized in identification of non-peptide masses that are bound to peptides. The tool is capable of identifying xenobiotic mass adducts, in addition to PTMs that were uncharacterized in the search parameters. 

### Quality Control

Quality control should be a central aspect of any mass spectrometry-based study to ensure reproducibility of generated results.
There are two types of quality controls that can be conducted for any kind of mass spectrometry experiment.
First, QC approaches should monitor instruments themselves (e.g. HPLC and mass spectrometer), and second, QC approaches should assess the quality of data from unknowns or samples. 
For further reading, an entire issue was published on quality control in the journal _Proteomics_ in 2011 [@DOI:10.1002/pmic.201190020], especially the review by Köcher _et al._ [@DOI:10.1002/pmic.201000578], as well as the review published by Bittremieux _et al._ in 2017 [@DOI:10.1002/mas.21544].

#### QC: Instrument Performance

It is generally advisable to monitor instrument performance regularly. 
Instrument calibrations in regular intervals are essential to ensure that performance is maintained.
Each instrument has a specific calibration method that is required.
During the calibration you can check injection times (for ion trap instruments) and intensity of the ions in the calibration mix.

After ensuring good calibration and signal with the simple calibration mixture, it is advisable to analyze complex samples, such as commercial simple peptide mixtures or even whole tryptic digests of cell lysates (e.g. K562 standard from Promega).
The additional check with a complex sample ensures all aspects of the system are working together correctly, especially the liquid chromatography and emitter.
These digests should be analyzed after every instrument calibration and periodically between samples when acquiring more extensive batches.
Data measured from tryptic digests should be analyzed by the software of your choice and the numbers of identified peptide precursors and proteins can be compared with previous controls for consistency.

Another strategy is to analyze digested purified proteins, which easily enable discovery of retention time shifts and mass accuracy problems. 
It is advised that new practicioners perform this manually at first to understand their data; this can be done by manually by looking *m/z* values of your standard peptides across runs in Skyline or the vendor-specific software.
Looking at the intensity of the extracted peaks will help identify sensitivity fluctuations. 

Carry-over between different measurements can be identified from blank measurements which are subsequently analyzed with your search software of choice.
Blank measurements can be injections of different buffers, water or the starting conditions of your liquid chromatography. 
In case of increased detection of carry-over, injections with trifluoroethanol can be performed.

Another factor to take into consideration is the stability of your electrospray.
Electrospray stability tends to worsen over time as columns wear and accumulate contaminants, such as salts or detergents, which can influence the quality of the emitter or column tip. 
You will notice spray instabilities either in the total ion chromatogram (TIC) as thin spikes with short periods of no measured signal or if you install cameras at your ESI source.
Suboptimal spray conditions will usually result in droplets forming on the emitter, being released into the mass spectrometer (also referred to as "spitting"). 
Real-time quality control software (listed in tables 9 and 10 below) can help you identify instrument issues right away.

Table 9: Quality Control Software for raw file and real-time analysis.

|    Name    |         Supported instrument vendors        |            Website/Download           |            publication            |           Note               |
|:----------:|:-------------------------------------------:|:-------------------------------------:|:---------------------------------:|:----------------------------:|
| QuiC | Thermo Scientific, AB SCIEX, Agilent, Bruker, Waters |[QuiC](https://biognosys.com/resources/quic-manual/)|                      | requires Biognosys iRT peptides |
| AlphaPept |  Thermo Scientific, Bruker              | [AlphaPept](https://github.com/MannLabs/alphapept) | [@DOI:10.1101/2021.07.23.453379]    |                      |
| RawMeat 2.1 | Thermo Scientific | [RawMeat](https://proteomicsresource.washington.edu/protocols06/RawMeat_1007.exe) |      |          |
| rawDiag | Thermo Scientific | [rawDiag](https://github.com/fgcz/rawDiag) |   [@DOI:10.1021/acs.jproteome.8b00173]        |                 |
| rawrr | Thermo Scientific | [rawrr](https://github.com/fgcz/rawrr) |  [@DOI:10.1021/acs.jproteome.0c00866]    |                       |
| rawBeans |  Thermo or mzML   | [rawBeans](https://bitbucket.org/incpm/prot-qc/downloads/)|   [@DOI:10.1021/acs.jproteome.0c00956]     |                  |
| SIMPATIQCO | Thermo Scientific | [SIMPATIQCO](https://ms.imp.ac.at/index.php?action=simpatiqco) | [@DOI:10.1021/pr300163u]    |                     |
| QC-ART |                     | [QC-ART](https://github.com/stanfill/QC-ART) | [@DOI:10.1074/mcp.RA118.000648] |                      |
| SprayQc | Thermo Scientific, AB SCIEX, extensible to other instrumentation | [SprayQc](http://sourceforge.org/projects/sprayqc) | [@DOI:10.1021/pr201219e] |               |
| Metriculator |         | [Metriculator](http://github.com/princelab/metriculator) | [@DOI:10.1093/bioinformatics/btt510] |            |
| MassQC |               | [MassQC](https://massqc.proteomesoftware.com/) |                   |                     |
| OpenMS |               | [OpenMS](https://www.openms.de/) |  [@DOI:10.1038/nmeth.3959]   |                  |

Table 10: Search result QC.

|    Name    |       Website/Download/publication        |              publication        |              Note           |
|:----------:|:-----------------------------------------:|:-------------------------------:|:----------------------------:|
| MSStats |[MSStats](https://github.com/Vitek-Lab/MSstats) |  [@DOI:10.1093/bioinformatics/btu305]|   can use output from MaxQuant, Proteome Discoverer, Skyline, Progenesis, Spectronaut |
| MSStatsQC |[MSStatsQC](https://msstats.org/msstatsqc/)   |  [@DOI:10.1074/mcp.M116.064774]    |               |
| PTXQC |[PTXQC](https://github.com/cbielow/PTXQC) |[@DOI:10.1021/acs.jproteome.5b00780] | requires MaxQuant search engine output | 
| protti | [protti](https://github.com/jpquast/protti) | [@DOI:10.1093/bioadv/vbab041]      |                   |

#### QC: Samples
Apart from instrument performance, data analysis should start with QC to identify problematic measurements and to exclude them if necessary. 
It is recommended to develop a standardized system for data QC early on and to keep this consistent over time. 
Adding indexed retention time (iRT) peptides to samples can help identify and correct gradient and retention time inconsistencies between samples at the data analysis stage.
Including common contaminants in FASTA files, such as keratins, is not considered optional because these contaminants are always present, and monitoring them can help identify sample preparation issues that may be non-uniform across samples.
Other parameters to check in your analysis are the consistency of the number of peptide-spectrum matches, identified peptides and proteins over all samples, as well as coefficients of variation between your replicates.
Before and after data normalization (if normalization is performed) it is good to compare the median intensities of all measurements to identify potential measurement or normalization issues. 
Precursor charge distributions, missed cleavage numbers, peak width, as well as the number of points per peak are additional parameters that can be checked.
In case you are analyzing different conditions, perform hierarchical clustering or a principal component analysis to check if your samples cluster as expected.

### Quantitative Proteomic Data Analysis Overview
This section aims to provide an overview of the best practices when conducting large scale proteomics quantitative data analysis. 
A universal workflow for proteomic data analysis does not currently exist because the processing depends on specific attributes of each individual dataset [@DOI:10.1016/j.jprot.2018.12.004]. 
Analyzing proteomic data requires knowledge of a multitude of pre-processing techniques where order matters, and it can be challenging knowing where to start. 
This review will cover tools to reduce bias due to nonbiological variability, statistical methods to identify differential expression and machine learning (ML) methods for supervised or unsupervised interpretation of proteomic data. 
For a no-code option of all processing methods described in this section, we recommend Perseus [@DOI:10.1038/nmeth.3901]. 

#### Data Transformation
Peptide or protein quantities are generally assumed to be logarithm (log) transformed before any subsequent processing [@DOI:10.1016/j.biosystems.2022.104661; @DOI:10.1021/pr050300l;  @DOI:10.1093/bib/bbw095; @DOI:10.1186/1471-2105-13-S16-S5]. 
Log transformation allows data to more closely conform to a normal distribution and reduces the effect of highly abundant proteins [@DOI:10.1021/pr050300l]. 
Many normalization techniques also assume data to be symmetric, so log transformation should precede any downstream analysis in these cases [@DOI:10.1021/pr050300l]. 
If there are missing values present, a simple approach would be to use log(1+x) to avoid taking the log of zero. 
After the transformation, zero quantities will remain as zero and the other quantities should be large enough that adding one will have a minor effect. 

#### Data Normalization 
Data normalization, the process for adjusting data to have comparable distributions between samples, should almost always be performed prior to batch correction and any subsequent data analysis [@DOI:10.1021/pr401264n; @DOI:10.1021/pr050300l]. 
This is required when the assumption is that most proteins are not changed between conditions, which is not always true.
For example, in some studies of post translational modifications where kinases are inhibited, there may be true shifts in total signal between conditions, and normalization would mask those differences. 
This is also true in co-IP experiments where one condition may truly have many fewer binding partners. 
Normalization removes systematic bias in peptide/protein abundances that could mask true biological discoveries or give rise to false conclusions [@DOI:10.1038/s41587-022-01440-w]. 
Bias may be due to factors such as measurement errors and protein degradation [@DOI:10.1021/pr050300l], although the causes for these variations are often unknown [@DOI:10.1186/1471-2105-13-S16-S5]. 
As data scaling methods should be kept at a minimum [@DOI:10.15252/msb.202110240], a normalization technique well suited to address the nuances specific to one’s data should be selected. 
The assumptions for a given normalization technique should not be violated, otherwise choosing the wrong technique can lead to misleading conclusions [@DOI:10.1093/bib/bbx008].
There are a multitude of data normalization techniques and knowing the most suitable one for a dataset can be challenging.

Visualization of peptide or protein intensity distributions among samples is an important step prior to selecting a normalization technique. 
Normalization is suggested to be done on the peptide level [@DOI:10.15252/msb.202110240]. 
If the technical variability causes the peptide/protein abundances from each sample to be different by a constant factor, and thus intensities are graphically similar across samples, then a central tendency normalization method such as mean, median or quantile normalization may be sufficient [@DOI:10.15252/msb.202110240; @DOI:10.1021/pr050300l]. 
However, if there is a linear relationship between bias and the peptide/protein abundances, a different method may be more appropriate. 
To visualize linear and nonlinear trends due to bias, we can plot the data in a ratio versus intensity, or a M (minus) versus A (average), plot [@DOI:10.1021/pr050300l; @DOI:10.1093/bioinformatics/19.2.185].
Linear regression normalization is an available technique if bias is linearly dependent on peptide/protein abundance magnitudes [@DOI:10.1093/bib/bbw095; @DOI:10.1021/pr050300l]. 
Alternatively, local regression (LOESS) normalization assumes nonlinearity between protein intensity and bias [@DOI:10.1093/bib/bbw095]. 
Another method, removal of unwanted variation (RUV), uses information from negative controls and a linear mixed effect model to estimate unwanted noise, which is then removed from the data [@DOI:10.1038/nbt.2931].

If sample distributions are drastically different, for example due to different treatments or samples are obtained from various tissues, one must use a method that preserves the heterogeneity in the data, including information present in outliers, meanwhile reducing systematic bias [@DOI:10.15252/msb.202110240].
For example, Hidden Markov Model (HMM)-assisted normalization [@DOI:10.15252/msb.202110240], RobNorm [@DOI:10.1093/bioinformatics/btaa904] or EigenMS [@DOI:10.1093/bioinformatics/btp426] may be suitable for this type of data. 
These techniques assume error is only due to the batch and order of processing. 
The first method that addresses correlation of errors between compounds by using the information from the variation of one variable to predict another is systematic error removal using random forest (SERRF) [@DOI:10.1021/acs.analchem.8b05592]. 
SERRF, among 14 normalization methods, was the most effective in significantly reducing systematic error [@DOI:10.1021/acs.analchem.8b05592].

Studies aiming to compare these methods for omics data normalization have come to different conclusions. 
Ranking of different normalization methods can be done by assessing the percent decrease in median log2(standard deviation) and log2 pooled estimate of variance (PEV) in comparison to the raw data [@DOI:10.1074/mcp.M800514-MCP200]. 
One study found linear regression ranked the highest compared to central tendency, LOESS and quantile normalization for peptide abundance normalization for replicate samples with and without biological differences [@DOI:10.1021/pr050300l]. 
A paper comparing multiple normalization methods using a large proteomic dataset found that mean/median centering, quantile normalization and RUV had the highest known associations between proteins and clinical variables [@DOI:10.1016/j.biosystems.2022.104661]. 
Rather than individually implementing normalization techniques, which can be challenging for non-domain experts, there are several R and Python packages that automate mass spectrometry data analysis and visualization. 
These tools assist with making an appropriate selection of a normalization technique. 
For example, NormalyzerDE, an R package, includes several popular methods for normalization and differential expression analysis of LC-MS data [@DOI:10.1021/acs.jproteome.8b00523]. 
AlphaPeptStats [@DOI:10.1093/bioinformatics/btad461], a Python package, allows for comprehensive mass spectrometry data analysis, including normalization, imputation, batch correction, visualization, statistical analysis and graphical representations including heatmaps, volcano plots, and scatter plots. 
AlphaPeptStats allows for analysis of label-free proteomics data from several platforms (MaxQuant, AlphaPept, DIA-NN, Spectronaut, FragPipe) in Python but also has web version that does not require installation. 
For both transformation and normalization, we recommend using the options in scikit-learn in python. 

#### Data Imputation
Missing peptide intensities, which are common in proteomic data, may need to be addressed, although this is a controversial topic in the field. 
Normalization should be performed before imputation since bias may not be removed to detect group differences if imputation occurs prior to normalization [@DOI:10.1186/1471-2105-13-S16-S5]. 
Reasons for missing data include the peptide not being biologically present, being present but at too low of a quantity to be detected, or present at quantifiable abundance but misidentified or incorrectly undetected [@DOI:10.1186/1471-2105-13-S16-S5]. 
If the quantity is not at the detectable limit, the quantity is called censored and these values are missing not at random [@DOI:10.1186/1471-2105-13-S16-S5]. 
Imputing these censored values will lead to bias as the imputed values will be overestimated [@DOI:10.1186/1471-2105-13-S16-S5]. 
However, if the quantity is present at detectable limits but was missed due to a problem with the instrument, this peptide is missing completely at random (MCAR) [@DOI:10.1186/1471-2105-13-S16-S5]. 
While imputation of values that are MCAR using observed values would be a reasonable approach, censored peptides should not be imputed because their missingness is informative [@DOI:10.1186/1471-2105-13-S16-S5]. 
Peptides MCAR are a less frequent problem compared to censored peptides [@DOI:10.1186/1471-2105-13-S16-S5]. 
Understanding why the peptide is missing can be challenging [@DOI:10.1186/1471-2105-13-S16-S5], however there are techniques such as maximum likelihood model [@DOI:10.1093/bioinformatics/btp362] or logistic regression [@DOI:10.1007/s12561-009-9013-2] that may distinguish censored versus MCAR values.

Commonly used imputation methods for omics data are random forest (RF) imputation[@DOI:10.1093/bioinformatics/btr597], k-nearest neighbors (kNN) imputation [@DOI:10.1093/bioinformatics/17.6.520], and single value decomposition (SVD) [@DOI:10.1093/bioinformatics/btm069]. 
Using the mean or median of the non-missing values for a variable is an easy approach to imputation but may lead to underestimating the true biological differences [@DOI:10.1186/1471-2105-13-S16-S5]. 
Choice of the appropriate imputation method is critical as how these missing values are filled in has a substantial impact on downstream analysis and conclusions [@DOI:10.1038/s41598-017-19120-0]. 
In one study, RF imputation was the most accurate among nine imputation methods across several combinations of types and rates of missingness and does not require preprocessing (e.g., does not require normal distribution) for metabolomics data [@DOI:10.1186/s12859-019-3110-0]. 
Another study found RF, among eight imputation methods, had the lowest normalized root mean squared error (NRMSE) between imputed values and the actual values when MCAR values were randomly replaced with missing values, followed by SVD and KNN using metabolomics data [@DOI:10.1038/s41598-017-19120-0]. 
Lastly, a study found RF also had the lowest NRMSE when comparing seven imputation methods using a large-scale label-free proteomics dataset [@DOI:10.1038/s41598-021-81279-4].
In general for imputation, we recommend using missforest, which is available as both a R and python package. 

### Batch Correction 
Normalization is assumed to occur prior to batch effect correction [@DOI:10.15252/msb.202110240].
Batch effect correction is still a critical step after normalization as proteins may still be affected by batch effects and diagnosing a batch effect may be easier once data is normalized [@DOI:10.15252/msb.202110240]. 
Prior to performing any statistical analysis of data, we must start with distinguishing signals in the data due to biological versus batch effects. 
A batch effect occurs when differences in preparation of samples and how data was acquired between batches results in altered quantities of peptides (or genes or metabolites) which results in reduced statistical power in detecting true differences [@DOI:10.15252/msb.202110240; @DOI:10.15252/msb.202110240]. 
This non-biological variability originates from the time of sample collection to peptide/protein quantification [@DOI:10.1016/j.biosystems.2022.104661] and is often a problem when working with large numbers of samples, involving multiple plates run by different technicians, on different instruments and/or using different reagent batches [@DOI:10.1016/j.tibtech.2017.02.012]. 
Results from these different batches ultimately need to be aggregated and data analysis to be performed on the whole dataset, so it may be difficult to measure and then control for exact changes due to non-biological variability once the data has been aggregated [@DOI:10.1016/j.biosystems.2022.104661]. 
Batch correction methods remove technical variability, however they should not remove any true biological effect [@DOI:10.1016/j.tibtech.2017.02.012; @DOI:10.1016/j.biosystems.2022.104661]. 
Although it is agreed upon that these biases should be accounted for to prevent misleading conclusions, there is no one gold standard batch correction method. 

Batch effects can manifest as continuous, such as from MS signal drift, or as discrete, such as a shift that affects the entire batch [@DOI:10.15252/msb.202110240]. 
To visualize batch effects, one can plot the average intensity per sample in the order each was measured by the MS to see if intensities are shifted in a certain batch [@DOI:10.15252/msb.202110240]. 
Measuring protein-protein correlations is another method to check for batch effects; if proteins within a batch are more correlated compared to those from other batches, there are likely batch effects[@DOI:10.15252/msb.202110240]. 
Prior to batch correction, one should ensure the experimental design is not inherently flawed due to batch effects and whether a change in design should be implemented. 
Studies spanning multiple days and experiments involving samples from different centers are vulnerable to batch effects [@DOI:10.1038/nrg2825]. 
One example of technical variability that may irreversibly flaw an experiment would be running samples at varying time points, or ‘as they came in’ [@DOI:10.1093/bfgp/3.4.322]. 
This problem can be circumvented by balancing biological groups in each batch [@DOI:10.1093/bfgp/3.4.322]. 
Additionally, collection of samples at different institutions introduces non-biological variability due to differences in a multitude of conditions such as collection protocols, storage, and transportation [@DOI:10.1016/j.biosystems.2022.104661]. 
A solution to this problem would be to evenly distribute samples between centers or batches [@DOI:10.1016/j.biosystems.2022.104661]. 

There are several batch correction methods, the most popular method being Combating Batch Effects When Combining Batches of Gene Expression Microarray Data (ComBat), originally designed for genomics data [@DOI:10.1093/biostatistics/kxj037; @DOI:10.1016/j.tibtech.2017.02.012]. 
ComBat uses Bayesian inference to estimate batch effects across features in a batch and applies a modified mean shift, but requires peptides to be present in all batches which can lead to loss of a large number of peptides [@DOI:10.15252/msb.202110240, @DOI:10.1016/j.tibtech.2017.02.012]. 
Combat is available as a python package called pycombat. 
Out of six batch correction methods using microarray data, ComBat was the best in reducing batch effects across several performance metrics and was effective using high dimensional data with small sample sizes [@DOI:10.1371/journal.pone.0017238]. 
ComBat may be more suitable for small datasets when the source of batch effects are known [@DOI:10.1016/j.tibtech.2017.02.012]. 
However if potential batch variables are not known or processing time or group does not adequately control for batch effects, surrogate variable analysis (SVA) may be used where the source of batch effect is estimated from the data [@DOI:10.1016/j.tibtech.2017.02.012; @DOI:10.1038/nrg2825]. 
A third option for batch effect correction uses negative control proteins to estimate unwanted variation, called “Remove Unwanted Variation, 2-step” (RUV-2) [@DOI:10.1093/biostatistics/kxr034].
There are many additional batch effect correction methods for single cell data, such as mutual nearest neighbors [@DOI:10.1038/nbt.4091], or Scanorama, which generalizes mutual nearest neighbors matching [@DOI:10.1038/s41587-019-0113-3].

### Assessment of Transformed Data
Prior to conducting any statistical analysis, the raw data matrix should be compared to the data after the above-described pre-processing steps have been performed to ensure bias is removed. 
We can compare data using boxplots of peptide intensities from the raw data matrix versus corrected data in sample running order to look at batch associated patterns; after correction, we should see uniform intensities across batches [@DOI:10.15252/msb.202110240]. 
We can also use clustering methods such as Principal Component analysis (PCA), Uniform Manifold Approximation and Projection (UMAP), or t-SNE (t-Distribute Stochastic Neighbor Embedding) and plot protein quantities colored by batches or technical versus biological samples to see how proteins cluster in space based on similarity. 
We can measure the variability each PC contributes; we want to see similar variability among all PCs, however if see one PC contributing to overall variability highly then means variables are dependent [@DOI:10.1002/pmic.202100103]. 
tSNE and UMAP allow for non-linear transformations and allow for clusters to be more visually distinct [@DOI:10.1002/pmic.202100103]. 
Grouping of similar samples by batch or other non-biological factors, such as time or plate, indicates bias [@DOI:10.15252/msb.202110240]. 
Quantitative measures of whether batch effects have been removed are principal variance components analysis (PVCA), which provides information on factors driving variance between biological and technical differences, and checking correlation of samples between different batches, within the same batch and between replicates. 
When batch effects are present, samples in the same batch will have higher correlation than samples from different batches and between replicates [@DOI:10.15252/msb.202110240]. 
Once batch effects are removed, proteins in the same batch should be correlated at the same level with proteins from other batches [@DOI:10.15252/msb.202110240]. 
Similarity between technical replicates can be measured using pooled median absolute deviation (PMAD), pooled coefficient of variation (PCV) and pooled estimate of variance (PEV); high similarity would mean batch effects are removed and there is low non-biological effects [@DOI:10.1093/bib/bbw095].

Lastly, it is also important to show that batch correction leads to improvement in finding true biological differences between samples. 
We can show the positive effect that batch correction has on the data by demonstrating reproducibility after batch correction. 
One way to provide evidence for reproducibility is to show that prior to batch correction, there was no overlap between differentially expressed proteins between groups in one batch with those found between the same groups in another batch and, after batch correction, the differentially expressed proteins between the groups become the same between batches [@DOI:10.15252/msb.202110240]. 
This applies generally datasets with large numbers (e.g., hundreds) of samples to allow for meaningful statistical comparisons [@DOI:10.15252/msb.202110240].

### Statistical Analysis
Once the above pre-processing steps have been applied to the dataset, we can investigate if any protein quantities differ between groups. 
There is an urgent need for biomarkers for disease prediction and there is large potential for protein based biomarker candidates [@DOI:10.1016/j.cels.2021.06.006]. 
However, omics datasets are often limited due to having many more features than number of samples, which is termed the ‘curse of dimensionality’[@DOI:10.1016/j.jprot.2018.12.004]. 
Attributes that are redundant or not informative can reduce the accuracy of a model [@DOI:10.1089/omi.2013.0017]. 

Univariate statistical tests including t-tests and analysis of variance (ANOVA) provide p-values to allow ranking the importance of variables [@DOI:10.1016/j.jprot.2018.12.004]. 
T-tests are used in pairwise comparisons, and ANOVA is used when there are multiple groups to ask whether any group is different from the rest. 
After ANOVA, the Tukey’s posthoc test can reveal which pairwise differences are present among the multiple groups that were compared. 
There are many posthoc tests that can be used, and guidance from an expert statistician is suggested.
Wilcoxon rank-sum tests can be used if the data are still not normal after the above approaches and therefore violates the assumptions required for a t-test.
Kruskal-Wallis test is the non-parametric version of ANOVA useful for three or more groups when assumptions of ANOVA are violated.  

Data can be reduced using a feature selection method, which includes either feature subset selection, where irrelevant features are removed, or feature extraction, where there is a transformation that generates new, aggregated variables and do not lead to loss of information [@DOI:10.1016/j.jprot.2018.12.004]. 
An example of a commonly used multivariate feature extraction method using proteomic data is principal component analysis (PCA) [@DOI:10.1016/j.jprot.2018.12.004]. 

Multiple hypothesis test happens in proteomics when we make many statistical tests to check for differences of many measured proteins betweenc conditions. 
For example, if we measure 1000 proteins between a pair of conditions, we may perform 1000 t-tests. 
By random chance, even in the absence of true protein quantity changes, 10 of these tests will produce a p-value less than 0.01. 
These would be false positives (i.e., a p-value will appear significant by chance) [@DOI:10.1002/pmic.202100103] and multiple testing correction should be applied to main the overall false positive rate at less than a specified cut-off [@DOI:10.1016/j.jprot.2018.12.004]. 
There are many methods for multiple testing correction.
Benjamini-Hochberg correction is less stringent than the Bonferroni correction, which leads to too many false negatives, and thus is a more commonly used multiple testing correction method [@DOI:10.1016/j.jprot.2018.12.004]. 

Volcano plots allow visualization of differentially abundant proteins by displaying the negative log of the adjusted p-value as a function of the log fold change, a measure of effect size, for each protein. 
Points with larger y axis values are more statistically significant and those further away from zero on the x axis have a larger fold change.  
There are two methods for identifying differentially expressed proteins. 
The first method involves a combined adjusted p-value cut-off (y axis) and fold change cut-off (x axis) to create a ‘square cut-off’[@DOI:10.1002/pmic.202100103]. 
The second involves a non-linear cut-off, where a systematic error is added to all the standard deviations used in the t-tests [@DOI:10.1002/pmic.202100103]. 

There are other statistical tests to consider for quantitative proteomics data. 
Another popular statistical method in proteomics when dealing with high dimensional data is lasso linear regression, which removes regression coefficients from the model by applying a penalty parameter [@DOI:10.1016/j.euprot.2015.08.001]. 
Bayesian models are an emerging technique for protein based biomarker discovery that are more powerful than standard t-tests [@DOI:10.1016/j.euprot.2015.08.001] and have outperformed linear models [@DOI:10.1016/j.euprot.2015.08.001; @DOI:10.1186/1471-2105-11-177]. 
Bayesian models incorporate external information into the prior distribution; for example, knowledge of peptides that usually have more technical variability are assigned a less informative prior [@DOI:10.1016/j.euprot.2015.08.001]. 
Prior to implementing machine learning (ML), one can start with the simpler models, such as linear regression or naïve Bayes [@DOI:10.1016/j.cels.2021.06.006].  

In general for statistical tests, can suggest using the standard test available in base R or in python packages scipy or statsmodels. 


### Machine Learning (ML) 
Despite the efficacy of ML for finding signals in a high dimensional feature space to distinguish between classes [@DOI:10.1016/j.cels.2021.06.006], the application of ML to proteomic data analysis is still in its early stages [@DOI:10.1016/j.cels.2021.06.006] as only 2% of proteomics studies involve ML [@DOI:10.1016/j.xcrp.2022.101069]. 
The reason for such sparse usage of ML in proteomics data is the need for very large datasets comprising 100s of samples, which is still rare in proteomics. 

Supervised classification is the most common type of ML used for proteomic biomarker discovery, where an algorithm has been trained on variables to predict the class labels of unseen test data [@DOI:10.1089/omi.2013.0017]. 
Supervised means the class labels, such as disease versus controls, is known [@DOI:10.1016/j.xcrp.2022.101069]. 
Decision trees are common model choice due to their many advantages: variables are not assumed to be linearly related, models are able to rank more important variables on their own, and interactions between variables do not need to be pre-specified by the user [@DOI:10.1016/j.euprot.2015.08.001]. 
There are three phases of model development and evaluation [@DOI:10.1038/s41598-022-09954-8]. 
In the first step, the dataset is split into training and testing splits, commonly 70% training and 30% testing. 
Second, the model is constructed using only the training data, which is further subdivided into training and test sets. 
During this process, an internal validation strategy, or cross-validation (CV), is employed [@DOI:10.1016/j.jprot.2018.12.004]. 
Commonly used CV methods in proteomics are k-fold and leave-one-out cross-validation[@DOI:10.1016/j.jprot.2018.12.004]. 
The final step is to evaluate the model on the testing set that was held-out in step one. 
There should not be overlap between the training and testing data, and the testing data should only be evaluated once after all training has been completed. 
The dataset used for training and testing should be representative of the population that is to be eventually tested. 
If underrepresented groups are lacking from models during training, these models will not generalize to these populations [@DOI:10.3233/JAD-201318]. 
Proteomic data and patient specific factors derived from the electronic health record (EHR) like age, race, and smoking status can be employed as inputs to a model [@DOI:10.1016/j.cels.2021.06.006]. 
However, addition of EHR data may not be informative in some instances; in studying Alzheimer’s Disease, adding these patient specific variables were informative for non-Hispanic white participants, but not for African Americans [@DOI:10.3233/JAD-201318]. 

A common mistake in proteomics ML studies is allowing the test data to leak into the feature selection step [@DOI:10.1021/acs.jproteome.2c00117; @DOI:10.1016/j.xcrp.2022.101069]. 
It has been reported that 80% of ML studies on the gut microbiome performed feature selection using all the data, including test data [@DOI:10.1021/acs.jproteome.2c00117]. 
Including the testing data in the feature selection step leads to development of an artificially inflated model [@DOI:10.1021/acs.jproteome.2c00117] that is overfit on the training data and performs poorly on new data [@DOI:10.1016/j.cels.2021.06.006]. 
Feature selection should occur only on the training set and final model performance should be reported using the unseen testing set. 
The number of samples should be ten times the number of features to make statistically valid comparisons, however this may not be possible in many cases [@DOI:10.1021/acs.jproteome.2c00117, @DOI:10.1038/nrc1322].
If a study is limited by its number of samples, one can perform classification without feature selection [@DOI:10.1016/j.xcrp.2022.101069, @DOI:10.1021/acs.jproteome.2c00117]. 

Pitfalls also arise when a ML classifier is trained using an imbalanced dataset [@DOI:10.1038/s41598-022-09954-8]. 
Proteomics biomarker studies commonly have imbalanced groups, where the number of samples in one group is drastically different from another group. 
Most ML algorithms assume balanced number of samples per class and not accounting for these differences can lead to reduced performance and construction of a biased classifier [@DOI:10.1371/journal.pone.0271260]. 

Care should be practiced when choosing an appropriate metric when dealing with imbalanced data. 
A high accuracy may be meaningless in the case of imbalanced classification; the number of correction predictions will be high even with a blind guess for the majority class [@DOI:10.1093/jamia/ocac093]. 
F1 score, Matthews correlation coefficient (MCC), and area under the precision recall curve (AUPR) are preferred metrics for imbalanced data classification [@DOI:10.1016/j.healun.2021.01.1160; @DOI:10.1371/journal.pone.0118432].  
MCC, for example, is preferred since it is only high if the model predicts correctly on both the positive and negative classes [@DOI:10.1038/s41598-022-09954-8]. 
Over- and under-sampling to equalize the number of samples in classes are potential methods to address class imbalance, but can be ineffective or even detrimental to the performance of the model [@DOI:10.1371/journal.pone.0271260]. 
These sampling methods may lead to a poorly calibrated model that overestimates the probability of the minority class samples and reduce the model’s applicability to clinical practice [@DOI:10.1093/jamia/ocac093]. 




